{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917d940f-b762-460d-abfd-50eb1b77006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql import Column, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# See lib.rs for details about constants\n",
    "CARD_TYPES = (\"DC\", \"CC\")\n",
    "TRANSACTION_TYPES = (\n",
    "    \"food-and-household\",\n",
    "    \"home\",\n",
    "    \"uncategorized\",\n",
    "    \"leisure-and-lifestyle\",\n",
    "    \"health-and-beauty\",\n",
    "    \"shopping-and-services\",\n",
    "    \"children\",\n",
    "    \"vacation-and-travel\",\n",
    "    \"education\",\n",
    "    \"insurance\",\n",
    "    \"investments-and-savings\",\n",
    "    \"expenses-and-other\",\n",
    "    \"cars-and-transportation\",\n",
    ")\n",
    "CHANNELS = (\"mobile\", \"web\")\n",
    "\n",
    "\n",
    "# Required time windows\n",
    "WINDOWS_IN_DAYS = (\n",
    "    7,  # week\n",
    "    14,  # two weeks\n",
    "    21,  # three weeks\n",
    "    30,  # month\n",
    "    90,  # three months\n",
    "    180,  # half of the year\n",
    "    360,  # two years\n",
    ")\n",
    "\n",
    "\n",
    "def get_all_aggregations(col_prefix: str, cond: Column, cols_list: list[Column]) -> None:\n",
    "    # Count over group\n",
    "    cols_list.append(\n",
    "        F.sum(F.when(cond, F.lit(1)).otherwise(F.lit(0))).alias(f\"{col_prefix}_count\")\n",
    "    )\n",
    "    # Average over group\n",
    "    cols_list.append(\n",
    "        F.mean(F.when(cond, F.col(\"trx_amnt\")).otherwise(F.lit(None))).alias(\n",
    "            f\"{col_prefix}_avg\"\n",
    "        )\n",
    "    )\n",
    "    # Sum over group\n",
    "    cols_list.append(\n",
    "        F.sum(F.when(cond, F.col(\"trx_amnt\")).otherwise(F.lit(0))).alias(\n",
    "            f\"{col_prefix}_sum\"\n",
    "        )\n",
    "    )\n",
    "    # Min over group\n",
    "    cols_list.append(\n",
    "        F.min(F.when(cond, F.col(\"trx_amnt\")).otherwise(F.lit(None))).alias(\n",
    "            f\"{col_prefix}_min\"\n",
    "        )\n",
    "    )\n",
    "    # Max over group\n",
    "    cols_list.append(\n",
    "        F.max(F.when(cond, F.col(\"trx_amnt\")).otherwise(F.lit(None))).alias(\n",
    "            f\"{col_prefix}_max\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10408902-573d-4e0e-8c55-11dfe92bb59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '../tmp_out': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Rm an output parquet folder if it exists\n",
    "!rm -r \"../tmp_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a5c56c-ea88-4613-ba44-fea760cc6624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:12:13 WARN Utils: Your hostname, toolbox resolves to a loopback address: 127.0.0.1; using 192.168.0.29 instead (on interface wlp0s20f3)\n",
      "24/05/11 19:12:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/11 19:12:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/11 19:12:23 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/05/11 19:12:24 WARN DAGScheduler: Broadcasting large task binary with size 1651.5 KiB\n",
      "[Stage 1:>                                                        (0 + 12) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.348s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 1.0 (TID 5): Retried waiting for GCLocker too often allocating 256 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 19:39:56 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"../test_data_small\"\n",
    "start_time = time.time()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", 2)\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "# root\n",
    "# |-- customer_id: long (nullable = true)\n",
    "# |-- card_type: string (nullable = true)\n",
    "# |-- trx_type: string (nullable = true)\n",
    "# |-- channel: string (nullable = true)\n",
    "# |-- trx_amnt: double (nullable = true)\n",
    "# |-- t_minus: long (nullable = true)\n",
    "# |-- part_col: string (nullable = true)\n",
    "\n",
    "data = spark.read.parquet(path)\n",
    "\n",
    "cols_list = []\n",
    "for win in WINDOWS_IN_DAYS:\n",
    "    # Iterate over combination card_type + trx_type\n",
    "    for card_type in CARD_TYPES:\n",
    "        for trx_type in TRANSACTION_TYPES:\n",
    "            cond = F.lit(True)\n",
    "            cond &= F.col(\"t_minus\") <= F.lit(win)  # Is row in the window?\n",
    "            cond &= F.col(\"card_type\") == F.lit(\n",
    "                card_type\n",
    "            )  # Does row have needed card type?\n",
    "            cond &= F.col(\"trx_type\") == F.lit(\n",
    "                trx_type\n",
    "            )  # Does row have needed trx type?\n",
    "\n",
    "            # Colname prefix\n",
    "            col_prefix = f\"{card_type}_{trx_type}_{win}d\"\n",
    "\n",
    "            get_all_aggregations(col_prefix, cond, cols_list)\n",
    "\n",
    "    # Iterate over combination channel + trx_type\n",
    "    for ch_type in CHANNELS:\n",
    "        for trx_type in TRANSACTION_TYPES:\n",
    "            cond = F.lit(True)\n",
    "            cond &= F.col(\"t_minus\") <= win  # Is row in the window?\n",
    "            cond &= F.col(\"channel\") == F.lit(\n",
    "                ch_type\n",
    "            )  # Does row have needed channel type?\n",
    "            cond &= F.col(\"trx_type\") == F.lit(\n",
    "                trx_type\n",
    "            )  # Does row have needed trx type?\n",
    "\n",
    "            # Colname prefix\n",
    "            col_prefix = f\"{ch_type}_{trx_type}_{win}d\"\n",
    "\n",
    "            get_all_aggregations(col_prefix, cond, cols_list)\n",
    "\n",
    "result = data.groupBy(\"customer_id\").agg(*cols_list)\n",
    "\n",
    "result.write.mode(\"overwrite\").parquet(\"../tmp_out\")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2514d7-a8df-458e-b3a4-67311b1af83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1692.9119520187378 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
